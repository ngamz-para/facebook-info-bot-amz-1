import time
import requests
import re
from bs4 import BeautifulSoup
from datetime import datetime

class FacebookScraperImproved:
    def __init__(self):
        # C·∫¢I TI·∫æN QUAN TR·ªåNG: Th√™m cookie v√† headers gi·ªëng tr√¨nh duy·ªát th·∫≠t h∆°n
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,vi;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
        }
        # Kh·ªüi t·∫°o session v·ªõi cookie c∆° b·∫£n
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        # Th√™m m·ªôt s·ªë cookie ph·ªï bi·∫øn ƒë·ªÉ tr√¥ng gi·ªëng l∆∞·ª£t truy c·∫≠p ƒë·∫ßu ti√™n
        self.session.cookies.update({
            'locale': 'en_US',
            'sb': 'placeholder_cookie_for_initial_request',
        })
    
    def scrape_fast(self, username):
        """
        Phi√™n b·∫£n ƒë√£ s·ª≠a: C·ªë g·∫Øng v∆∞·ª£t qua ch·∫∑n c∆° b·∫£n c·ªßa Facebook.
        """
        # C·∫¢I TI·∫æN: Th·ª≠ v·ªõi URL r√∫t g·ªçn 'fb.com' v√† ƒë∆∞·ªùng d·∫´n 'profile' tr∆∞·ªõc
        urls_to_try = [
            f"https://www.facebook.com/{username}",
            f"https://fb.com/{username}",
            f"https://www.facebook.com/profile.php?id={username}" if username.isdigit() else None,
        ]
        
        for url in filter(None, urls_to_try):
            print(f"üîç ƒêang th·ª≠ v·ªõi URL: {url}")
            result = self._try_scrape_url(url, username)
            if result.get('success'):
                return result
            # N·∫øu th·∫•t b·∫°i v√† b·ªã chuy·ªÉn h∆∞·ªõng ƒë·∫øn login, d·ª´ng th·ª≠ c√°c URL kh√°c
            if "login" in result.get('error', ''):
                return self._error_response(f"Truy c·∫≠p b·ªã ch·∫∑n. Facebook y√™u c·∫ßu ƒëƒÉng nh·∫≠p ƒë·ªÉ xem trang n√†y.")
        
        # N·∫øu t·∫•t c·∫£ ƒë·ªÅu th·∫•t b·∫°i
        return self._error_response("Kh√¥ng th·ªÉ truy c·∫≠p trang. C√≥ th·ªÉ trang kh√¥ng t·ªìn t·∫°i ho·∫∑c y√™u c·∫ßu ƒëƒÉng nh·∫≠p.")
    
    def _try_scrape_url(self, url, username):
        """Th·ª≠ thu th·∫≠p t·ª´ m·ªôt URL c·ª• th·ªÉ."""
        try:
            start_time = time.time()
            # QUAN TR·ªåNG: T·∫Øt t·ª± ƒë·ªông redirect ƒë·ªÉ ki·ªÉm tra n·∫øu Facebook chuy·ªÉn h∆∞·ªõng sang login
            response = self.session.get(url, timeout=15, allow_redirects=False)
            get_time = time.time() - start_time
            
            # Ki·ªÉm tra m√£ tr·∫°ng th√°i HTTP
            if response.status_code == 302 or response.status_code == 301:
                location = response.headers.get('Location', '')
                if 'login' in location:
                    # B·ªã chuy·ªÉn h∆∞·ªõng ƒë·∫øn trang ƒëƒÉng nh·∫≠p -> b·ªã ch·∫∑n
                    return self._error_response(f"B·ªã chuy·ªÉn h∆∞·ªõng t·ªõi login ({location})")
                else:
                    # Chuy·ªÉn h∆∞·ªõng kh√°c, c√≥ th·ªÉ th·ª≠ theo d√µi
                    pass
            elif response.status_code != 200:
                return self._error_response(f"M√£ l·ªói HTTP: {response.status_code}")
            
            # N·∫øu kh√¥ng b·ªã ch·∫∑n, ti·∫øp t·ª•c ph√¢n t√≠ch HTML
            html_content = response.text
            
            # KI·ªÇM TRA QUAN TR·ªåNG: Xem HTML c√≥ ch·ª©a t·ª´ kh√≥a "login" kh√¥ng
            if 'login' in html_content.lower() and 'password' in html_content.lower():
                return self._error_response("Trang tr·∫£ v·ªÅ l√† trang ƒëƒÉng nh·∫≠p (b·ªã ch·∫∑n).")
            
            print(f"‚úÖ T·∫£i HTML th√†nh c√¥ng t·ª´ {url} trong {get_time:.2f}s")
            soup = BeautifulSoup(html_content, 'lxml')
            
            info = self._extract_from_meta(soup, username, url)
            self._extract_detailed_info(soup, info)
            
            if info.get('uid') and info['uid'].isdigit():
                info['estimated_join_date'] = self._estimate_join_date(info['uid'])
            
            info.update({
                'scraped_in': f"{get_time:.2f}s",
                'success': True,
                'timestamp': datetime.now().strftime('%d/%m/%Y %H:%M:%S')
            })
            return info
            
        except requests.exceptions.Timeout:
            return self._error_response("Timeout: Facebook ph·∫£n h·ªìi qu√° ch·∫≠m")
        except requests.exceptions.RequestException as e:
            # C·∫¢I TI·∫æN: Ph√¢n t√≠ch l·ªói chi ti·∫øt h∆°n
            error_msg = str(e)
            if '400' in error_msg or '403' in error_msg:
                return self._error_response(f"Truy c·∫≠p b·ªã t·ª´ ch·ªëi (L·ªói {error_msg}). Facebook c√≥ th·ªÉ ƒë√£ ch·∫∑n IP c·ªßa server.")
            return self._error_response(f"L·ªói k·∫øt n·ªëi: {error_msg}")
        except Exception as e:
            return self._error_response(f"L·ªói x·ª≠ l√Ω: {str(e)}")

    def _extract_from_meta(self, soup, username, url):
        """Tr√≠ch xu·∫•t th√¥ng tin t·ª´ th·∫ª meta (nhanh v√† ·ªïn ƒë·ªãnh nh·∫•t)"""
        info = {
            'username': username,
            'url': url,
            'name': 'Kh√¥ng x√°c ƒë·ªãnh',
            'avatar_url': '',
            'uid': 'Kh√¥ng x√°c ƒë·ªãnh',
            'bio': '',
            'verified': 'Kh√¥ng'  # M·∫∑c ƒë·ªãnh l√† Kh√¥ng
        }
        
        # T√¨m t√™n t·ª´ og:title
        meta_title = soup.find('meta', property='og:title')
        if meta_title:
            full_title = meta_title.get('content', '')
            # T√°ch t√™n th·∫≠t t·ª´ title (lo·∫°i b·ªè " | Facebook")
            info['name'] = full_title.split('|')[0].strip()
        
        # T√¨m ·∫£nh ƒë·∫°i di·ªán t·ª´ og:image
        meta_image = soup.find('meta', property='og:image')
        if meta_image:
            info['avatar_url'] = meta_image.get('content', '')
        
        # T√¨m UID t·ª´ nhi·ªÅu ngu·ªìn kh√°c nhau trong HTML
        uid = self._find_uid_in_html(str(soup))
        if uid:
            info['uid'] = uid
        
        # T√¨m m√¥ t·∫£ bio
        meta_desc = soup.find('meta', property='og:description')
        if meta_desc:
            info['bio'] = meta_desc.get('content', '')[:150]
        
        return info
    
    def _find_uid_in_html(self, html):
        """T√¨m UID b·∫±ng nhi·ªÅu regex pattern (tƒÉng ƒë·ªô ch√≠nh x√°c)"""
        patterns = [
            r'"userID":"(\d+)"',           # Pattern c≈©
            r'"actor_id":(\d+)',           # Pattern m·ªõi
            r'profile_id=(\d+)',           # Trong URL
            r'/(\d+)/?$',                  # UID trong ƒë∆∞·ªùng d·∫´n
            r'content="fb://profile/(\d+)"' # Trong meta
        ]
        
        for pattern in patterns:
            match = re.search(pattern, html)
            if match:
                return match.group(1)
        return None
    
    def _extract_detailed_info(self, soup, info):
        """Tr√≠ch xu·∫•t th√¥ng tin chi ti·∫øt h∆°n t·ª´ HTML"""
        
        # C·∫¢I TI·∫æN: T√¨m tick xanh (verified) b·∫±ng nhi·ªÅu c√°ch
        verified = False
        
        # C√°ch 1: T√¨m bi·ªÉu t∆∞·ª£ng tick xanh qua SVG path
        svg_tags = soup.find_all('svg')
        for svg in svg_tags:
            if svg.find('path', {'d': True}):
                # Path data c·ªßa tick xanh th∆∞·ªùng c√≥ ch·ªØ "M18" ho·∫∑c ph·ª©c t·∫°p
                path_data = str(svg.find('path'))
                if 'M18' in path_data and ('9.5' in path_data or '12' in path_data):
                    verified = True
                    break
        
        # C√°ch 2: T√¨m trong alt text c·ªßa ·∫£nh
        img_tags = soup.find_all('img', alt=True)
        for img in img_tags:
            alt_text = img.get('alt', '').lower()
            if 'verified' in alt_text or 'ƒë√£ x√°c minh' in alt_text:
                verified = True
                break
        
        info['verified'] = 'C√≥ ‚úì' if verified else 'Kh√¥ng ‚úó'
        
        # T√¨m s·ªë ng∆∞·ªùi theo d√µi (followers)
        followers_text = 'Kh√¥ng c√¥ng khai'
        
        # T√¨m c√°c span c√≥ text li√™n quan ƒë·∫øn followers
        all_text = soup.get_text()
        followers_patterns = [
            r'(\d+[\.,]?\d*[KkM]?)\s*(ng∆∞·ªùi theo d√µi|followers)',
            r'(\d+[\.,]?\d*[KkM]?)\s*(l∆∞·ª£t theo d√µi)',
            r'Followers:\s*(\d+[\.,]?\d*[KkM]?)'
        ]
        
        for pattern in followers_patterns:
            match = re.search(pattern, all_text, re.IGNORECASE)
            if match:
                followers_text = f"{match.group(1)} ng∆∞·ªùi theo d√µi"
                break
        
        info['followers'] = followers_text
    
    def _estimate_join_date(self, uid):
        """
        ∆Ø·ªöC L∆Ø·ª¢NG ng√†y t·∫°o t√†i kho·∫£n d·ª±a tr√™n UID.
        ƒê√¢y l√† ph∆∞∆°ng ph√°p g·∫ßn ƒë√∫ng d·ª±a tr√™n quan s√°t.
        """
        try:
            uid_num = int(uid)
            
            # Facebook UID tƒÉng d·∫ßn theo th·ªùi gian
            # UID 4 (Mark Zuckerberg) ~ 2004
            # UID 100000xxx ~ 2008
            # ƒê√¢y l√† c√¥ng th·ª©c ∆Ø·ªöC L∆Ø·ª¢NG, kh√¥ng ch√≠nh x√°c 100%
            
            base_year = 2004
            base_uid = 4
            
            if uid_num <= base_uid:
                return "Kho·∫£ng 2004"
            
            # T√≠nh nƒÉm ∆∞·ªõc l∆∞·ª£ng (m·ªói 50 tri·ªáu UID ~ 1 nƒÉm)
            years_since_base = (uid_num - base_uid) / 50000000
            estimated_year = base_year + int(years_since_base)
            
            # Gi·ªõi h·∫°n nƒÉm trong kho·∫£ng h·ª£p l√Ω
            estimated_year = max(2004, min(estimated_year, datetime.now().year))
            
            return f"Kho·∫£ng nƒÉm {estimated_year}"
            
        except:
            return "Kh√¥ng th·ªÉ ∆∞·ªõc l∆∞·ª£ng"
    
    def _error_response(self, message):
        """T·∫°o response th√¥ng b√°o l·ªói"""
        return {
            'success': False,
            'error': message,
            'timestamp': datetime.now().strftime("%d/%m/Y %H:%M:%S")
        }

# H√†m wrapper ƒë·ªÉ bot.py g·ªçi
def get_facebook_info_improved(username):
    scraper = FacebookScraperImproved()
    return scraper.scrape_fast(username)
