import time
import requests
import re
from bs4 import BeautifulSoup
from datetime import datetime

class FacebookScraperImproved:
    def __init__(self):
        # C·∫¢I TI·∫æN QUAN TR·ªåNG: Th√™m cookie v√† headers gi·ªëng tr√¨nh duy·ªát th·∫≠t h∆°n
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9,vi;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
        }
        # Kh·ªüi t·∫°o session v·ªõi cookie c∆° b·∫£n
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        # Th√™m m·ªôt s·ªë cookie ph·ªï bi·∫øn ƒë·ªÉ tr√¥ng gi·ªëng l∆∞·ª£t truy c·∫≠p ƒë·∫ßu ti√™n
        self.session.cookies.update({
            'locale': 'en_US',
            'sb': 'placeholder_cookie_for_initial_request',
        })
    
    def scrape_fast(self, username):
        """
        Phi√™n b·∫£n ƒë√£ s·ª≠a: C·ªë g·∫Øng v∆∞·ª£t qua ch·∫∑n c∆° b·∫£n c·ªßa Facebook.
        """
        # C·∫¢I TI·∫æN: Th·ª≠ v·ªõi URL r√∫t g·ªçn 'fb.com' v√† ƒë∆∞·ªùng d·∫´n 'profile' tr∆∞·ªõc
        urls_to_try = [
            f"https://www.facebook.com/{username}",
            f"https://fb.com/{username}",
            f"https://www.facebook.com/profile.php?id={username}" if username.isdigit() else None,
        ]
        
        for url in filter(None, urls_to_try):
            print(f"üîç ƒêang th·ª≠ v·ªõi URL: {url}")
            result = self._try_scrape_url(url, username)
            if result.get('success'):
                return result
            # N·∫øu th·∫•t b·∫°i v√† b·ªã chuy·ªÉn h∆∞·ªõng ƒë·∫øn login, d·ª´ng th·ª≠ c√°c URL kh√°c
            if "login" in result.get('error', ''):
                return self._error_response(f"Truy c·∫≠p b·ªã ch·∫∑n. Facebook y√™u c·∫ßu ƒëƒÉng nh·∫≠p ƒë·ªÉ xem trang n√†y.")
        
        # N·∫øu t·∫•t c·∫£ ƒë·ªÅu th·∫•t b·∫°i
        return self._error_response("Kh√¥ng th·ªÉ truy c·∫≠p trang. C√≥ th·ªÉ trang kh√¥ng t·ªìn t·∫°i ho·∫∑c y√™u c·∫ßu ƒëƒÉng nh·∫≠p.")
    
    def _try_scrape_url(self, url, username):
        """Th·ª≠ thu th·∫≠p t·ª´ m·ªôt URL c·ª• th·ªÉ."""
        try:
            start_time = time.time()
            # QUAN TR·ªåNG: T·∫Øt t·ª± ƒë·ªông redirect ƒë·ªÉ ki·ªÉm tra n·∫øu Facebook chuy·ªÉn h∆∞·ªõng sang login
            response = self.session.get(url, timeout=15, allow_redirects=False)
            get_time = time.time() - start_time
            
            # Ki·ªÉm tra m√£ tr·∫°ng th√°i HTTP
            if response.status_code == 302 or response.status_code == 301:
                location = response.headers.get('Location', '')
                if 'login' in location:
                    # B·ªã chuy·ªÉn h∆∞·ªõng ƒë·∫øn trang ƒëƒÉng nh·∫≠p -> b·ªã ch·∫∑n
                    return self._error_response(f"B·ªã chuy·ªÉn h∆∞·ªõng t·ªõi login ({location})")
                else:
                    # Chuy·ªÉn h∆∞·ªõng kh√°c, c√≥ th·ªÉ th·ª≠ theo d√µi
                    pass
            elif response.status_code != 200:
                return self._error_response(f"M√£ l·ªói HTTP: {response.status_code}")
            
            # N·∫øu kh√¥ng b·ªã ch·∫∑n, ti·∫øp t·ª•c ph√¢n t√≠ch HTML
            html_content = response.text
            
            # KI·ªÇM TRA QUAN TR·ªåNG: Xem HTML c√≥ ch·ª©a t·ª´ kh√≥a "login" kh√¥ng
            if 'login' in html_content.lower() and 'password' in html_content.lower():
                return self._error_response("Trang tr·∫£ v·ªÅ l√† trang ƒëƒÉng nh·∫≠p (b·ªã ch·∫∑n).")
            
            print(f"‚úÖ T·∫£i HTML th√†nh c√¥ng t·ª´ {url} trong {get_time:.2f}s")
            soup = BeautifulSoup(html_content, 'lxml')
            
            info = self._extract_from_meta(soup, username, url)
            self._extract_detailed_info(soup, info)
            
            if info.get('uid') and info['uid'].isdigit():
                info['estimated_join_date'] = self._estimate_join_date(info['uid'])
            
            info.update({
                'scraped_in': f"{get_time:.2f}s",
                'success': True,
                'timestamp': datetime.now().strftime('%d/%m/%Y %H:%M:%S')
            })
            return info
            
        except requests.exceptions.Timeout:
            return self._error_response("Timeout: Facebook ph·∫£n h·ªìi qu√° ch·∫≠m")
        except requests.exceptions.RequestException as e:
            # C·∫¢I TI·∫æN: Ph√¢n t√≠ch l·ªói chi ti·∫øt h∆°n
            error_msg = str(e)
            if '400' in error_msg or '403' in error_msg:
                return self._error_response(f"Truy c·∫≠p b·ªã t·ª´ ch·ªëi (L·ªói {error_msg}). Facebook c√≥ th·ªÉ ƒë√£ ch·∫∑n IP c·ªßa server.")
            return self._error_response(f"L·ªói k·∫øt n·ªëi: {error_msg}")
        except Exception as e:
            return self._error_response(f"L·ªói x·ª≠ l√Ω: {str(e)}")
    
    # C√ÅC PH∆Ø∆†NG TH·ª®C _extract_from_meta, _find_uid_in_html, _extract_detailed_info, _estimate_join_date, _error_response
    # V·∫™N GI·ªÆ NGUY√äN NH∆Ø CODE C≈® C·ª¶A B·∫†N, T√îI S·∫º KH√îNG SAO CH√âP L·∫†I ·ªû ƒê√ÇY ƒê·ªÇ TR√ÅNH D√ÄI D√íNG.
    # B·∫°n ch·ªâ c·∫ßn gi·ªØ nguy√™n c√°c ph∆∞∆°ng th·ª©c n√†y t·ª´ file c≈© c·ªßa b·∫°n.

# H√†m wrapper ƒë·ªÉ bot.py g·ªçi
def get_facebook_info_improved(username):
    scraper = FacebookScraperImproved()
    return scraper.scrape_fast(username)
