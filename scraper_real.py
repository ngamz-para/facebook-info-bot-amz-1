import time
import requests
import re
from bs4 import BeautifulSoup
from datetime import datetime

class FacebookScraperImproved:
    def __init__(self):
        # Headers gi·∫£ m·∫°o tr√¨nh duy·ªát th·∫≠t
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
            'Connection': 'keep-alive',
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    def scrape_fast(self, username):
        """
        Phi√™n b·∫£n c·∫£i ti·∫øn: NHANH H∆†N v√† ch√≠nh x√°c h∆°n.
        D√πng requests + BeautifulSoup thay v√¨ Selenium khi c√≥ th·ªÉ.
        """
        url = f"https://www.facebook.com/{username}"
        print(f"üöÄ ƒêang thu th·∫≠p nhanh: {username}")
        
        try:
            # 1. L·∫§Y HTML B·∫∞NG REQUESTS (SI√äU NHANH)
            start_time = time.time()
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            html_content = response.text
            
            get_time = time.time() - start_time
            print(f"‚è±Ô∏è  T·∫£i HTML xong trong {get_time:.2f}s")
            
            # 2. PH√ÇN T√çCH V·ªöI BEAUTIFULSOUP
            soup = BeautifulSoup(html_content, 'lxml')
            
            # 3. L·∫§Y TH√îNG TIN C∆† B·∫¢N T·ª™ META TAGS
            info = self._extract_from_meta(soup, username, url)
            
            # 4. T√åM TH√îNG TIN CHI TI·∫æT H∆†N TRONG HTML
            self._extract_detailed_info(soup, info)
            
            # 5. ∆Ø·ªöC L∆Ø·ª¢NG NG√ÄY T·∫†O T√ÄI KHO·∫¢N (N·∫øu c√≥ UID)
            if info.get('uid') and info['uid'].isdigit():
                info['estimated_join_date'] = self._estimate_join_date(info['uid'])
            
            info['scraped_in'] = f"{get_time:.2f}s"
            info['success'] = True
            return info
            
        except requests.exceptions.Timeout:
            return self._error_response("Timeout: Facebook ph·∫£n h·ªìi qu√° ch·∫≠m")
        except requests.exceptions.RequestException as e:
            return self._error_response(f"L·ªói k·∫øt n·ªëi: {str(e)}")
        except Exception as e:
            return self._error_response(f"L·ªói x·ª≠ l√Ω: {str(e)}")
    
    def _extract_from_meta(self, soup, username, url):
        """Tr√≠ch xu·∫•t th√¥ng tin t·ª´ th·∫ª meta (nhanh v√† ·ªïn ƒë·ªãnh nh·∫•t)"""
        info = {
            'username': username,
            'url': url,
            'name': 'Kh√¥ng x√°c ƒë·ªãnh',
            'avatar_url': '',
            'uid': 'Kh√¥ng x√°c ƒë·ªãnh',
            'bio': '',
            'verified': 'Kh√¥ng'  # M·∫∑c ƒë·ªãnh l√† Kh√¥ng
        }
        
        # T√¨m t√™n t·ª´ og:title
        meta_title = soup.find('meta', property='og:title')
        if meta_title:
            full_title = meta_title.get('content', '')
            # T√°ch t√™n th·∫≠t t·ª´ title (lo·∫°i b·ªè " | Facebook")
            info['name'] = full_title.split('|')[0].strip()
        
        # T√¨m ·∫£nh ƒë·∫°i di·ªán t·ª´ og:image
        meta_image = soup.find('meta', property='og:image')
        if meta_image:
            info['avatar_url'] = meta_image.get('content', '')
        
        # T√¨m UID t·ª´ nhi·ªÅu ngu·ªìn kh√°c nhau trong HTML
        uid = self._find_uid_in_html(str(soup))
        if uid:
            info['uid'] = uid
        
        # T√¨m m√¥ t·∫£ bio
        meta_desc = soup.find('meta', property='og:description')
        if meta_desc:
            info['bio'] = meta_desc.get('content', '')[:150]
        
        return info
    
    def _find_uid_in_html(self, html):
        """T√¨m UID b·∫±ng nhi·ªÅu regex pattern (tƒÉng ƒë·ªô ch√≠nh x√°c)"""
        patterns = [
            r'"userID":"(\d+)"',           # Pattern c≈©
            r'"actor_id":(\d+)',           # Pattern m·ªõi
            r'profile_id=(\d+)',           # Trong URL
            r'/(\d+)/?$',                  # UID trong ƒë∆∞·ªùng d·∫´n
            r'content="fb://profile/(\d+)"' # Trong meta
        ]
        
        for pattern in patterns:
            match = re.search(pattern, html)
            if match:
                return match.group(1)
        return None
    
    def _extract_detailed_info(self, soup, info):
        """Tr√≠ch xu·∫•t th√¥ng tin chi ti·∫øt h∆°n t·ª´ HTML"""
        
        # C·∫¢I TI·∫æN: T√¨m tick xanh (verified) b·∫±ng nhi·ªÅu c√°ch
        verified = False
        
        # C√°ch 1: T√¨m bi·ªÉu t∆∞·ª£ng tick xanh qua SVG path
        svg_tags = soup.find_all('svg')
        for svg in svg_tags:
            if svg.find('path', {'d': True}):
                # Path data c·ªßa tick xanh th∆∞·ªùng c√≥ ch·ªØ "M18" ho·∫∑c ph·ª©c t·∫°p
                path_data = str(svg.find('path'))
                if 'M18' in path_data and ('9.5' in path_data or '12' in path_data):
                    verified = True
                    break
        
        # C√°ch 2: T√¨m trong alt text c·ªßa ·∫£nh
        img_tags = soup.find_all('img', alt=True)
        for img in img_tags:
            alt_text = img.get('alt', '').lower()
            if 'verified' in alt_text or 'ƒë√£ x√°c minh' in alt_text:
                verified = True
                break
        
        info['verified'] = 'C√≥ ‚úì' if verified else 'Kh√¥ng ‚úó'
        
        # T√¨m s·ªë ng∆∞·ªùi theo d√µi (followers)
        followers_text = 'Kh√¥ng c√¥ng khai'
        
        # T√¨m c√°c span c√≥ text li√™n quan ƒë·∫øn followers
        all_text = soup.get_text()
        followers_patterns = [
            r'(\d+[\.,]?\d*[KkM]?)\s*(ng∆∞·ªùi theo d√µi|followers)',
            r'(\d+[\.,]?\d*[KkM]?)\s*(l∆∞·ª£t theo d√µi)',
            r'Followers:\s*(\d+[\.,]?\d*[KkM]?)'
        ]
        
        for pattern in followers_patterns:
            match = re.search(pattern, all_text, re.IGNORECASE)
            if match:
                followers_text = f"{match.group(1)} ng∆∞·ªùi theo d√µi"
                break
        
        info['followers'] = followers_text
    
    def _estimate_join_date(self, uid):
        """
        ∆Ø·ªöC L∆Ø·ª¢NG ng√†y t·∫°o t√†i kho·∫£n d·ª±a tr√™n UID.
        ƒê√¢y l√† ph∆∞∆°ng ph√°p g·∫ßn ƒë√∫ng d·ª±a tr√™n quan s√°t.
        """
        try:
            uid_num = int(uid)
            
            # Facebook UID tƒÉng d·∫ßn theo th·ªùi gian
            # UID 4 (Mark Zuckerberg) ~ 2004
            # UID 100000xxx ~ 2008
            # ƒê√¢y l√† c√¥ng th·ª©c ∆Ø·ªöC L∆Ø·ª¢NG, kh√¥ng ch√≠nh x√°c 100%
            
            base_year = 2004
            base_uid = 4
            
            if uid_num <= base_uid:
                return "Kho·∫£ng 2004"
            
            # T√≠nh nƒÉm ∆∞·ªõc l∆∞·ª£ng (m·ªói 50 tri·ªáu UID ~ 1 nƒÉm)
            years_since_base = (uid_num - base_uid) / 50000000
            estimated_year = base_year + int(years_since_base)
            
            # Gi·ªõi h·∫°n nƒÉm trong kho·∫£ng h·ª£p l√Ω
            estimated_year = max(2004, min(estimated_year, datetime.now().year))
            
            return f"Kho·∫£ng nƒÉm {estimated_year}"
            
        except:
            return "Kh√¥ng th·ªÉ ∆∞·ªõc l∆∞·ª£ng"
    
    def _error_response(self, message):
        return {
            'success': False,
            'error': message,
            'timestamp': datetime.now().strftime("%d/%m/%Y %H:%M:%S")
        }

# H√†m wrapper ƒë·ªÉ bot.py g·ªçi
def get_facebook_info_improved(username):
    scraper = FacebookScraperImproved()
    return scraper.scrape_fast(username)
