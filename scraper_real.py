import time
import requests
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from bs4 import BeautifulSoup
import re

class FacebookScraper:
    def __init__(self, headless=True):
        """Kh·ªüi t·∫°o tr√¨nh duy·ªát Chrome v·ªõi c·∫•u h√¨nh headless"""
        self.options = Options()
        
        if headless:
            self.options.add_argument("--headless=new")
        
        # C√°c c·∫•u h√¨nh quan tr·ªçng cho server
        self.options.add_argument("--no-sandbox")
        self.options.add_argument("--disable-dev-shm-usage")
        self.options.add_argument("--disable-gpu")
        self.options.add_argument("--window-size=1920,1080")
        
        # Gi·∫£ m·∫°o user-agent
        self.options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
        
        # T·∫Øt th√¥ng b√°o Chrome
        self.options.add_experimental_option("excludeSwitches", ["enable-logging"])
        
        self.driver = None
        self.wait_timeout = 30
    
    def start_browser(self):
        """Kh·ªüi ƒë·ªông tr√¨nh duy·ªát"""
        try:
            self.driver = webdriver.Chrome(options=self.options)
            return True
        except WebDriverException as e:
            print(f"‚ùå L·ªói kh·ªüi ƒë·ªông Chrome: {e}")
            return False
    
    def scrape_basic_info(self, username):
        """
        Thu th·∫≠p th√¥ng tin C∆† B·∫¢N t·ª´ trang c√° nh√¢n Facebook
        CH·ªà HO·∫†T ƒê·ªòNG V·ªöI TRANG C√îNG KHAI
        """
        if not self.driver:
            if not self.start_browser():
                return self._create_error_response("Kh√¥ng th·ªÉ kh·ªüi ƒë·ªông tr√¨nh duy·ªát")
        
        try:
            # M·ªü trang Facebook (KH√îNG ƒëƒÉng nh·∫≠p)
            url = f"https://www.facebook.com/{username}"
            print(f"üîç ƒêang truy c·∫≠p: {url}")
            
            self.driver.get(url)
            
            # Ch·ªù trang t·∫£i - QUAN TR·ªåNG: Facebook c√≥ nhi·ªÅu redirect
            time.sleep(5)
            
            # Ki·ªÉm tra xem c√≥ ph·∫£i trang l·ªói kh√¥ng
            if "trang n√†y kh√¥ng kh·∫£ d·ª•ng" in self.driver.page_source.lower():
                return self._create_error_response("Trang kh√¥ng t·ªìn t·∫°i ho·∫∑c kh√¥ng c√¥ng khai")
            
            # L·∫•y HTML ƒë·ªÉ ph√¢n t√≠ch
            page_source = self.driver.page_source
            soup = BeautifulSoup(page_source, 'lxml')
            
            # ========== TR√çCH XU·∫§T TH√îNG TIN ==========
            info = {
                'success': True,
                'username': username,
                'url': url
            }
            
            # 1. T√¨m t√™n (th·∫ª meta og:title)
            meta_title = soup.find('meta', property='og:title')
            if meta_title:
                info['name'] = meta_title.get('content', '').split('|')[0].strip()
            else:
                # Fallback: t√¨m trong title
                title_tag = soup.find('title')
                if title_tag:
                    info['name'] = title_tag.text.split('|')[0].strip()
            
            # 2. T√¨m ·∫£nh ƒë·∫°i di·ªán (meta og:image)
            meta_image = soup.find('meta', property='og:image')
            if meta_image:
                info['avatar_url'] = meta_image.get('content', '')
            
            # 3. T√¨m UID t·ª´ source (n·∫øu c√≥)
            uid_match = re.search(r'"userID":"(\d+)"', page_source)
            if uid_match:
                info['uid'] = uid_match.group(1)
            else:
                info['uid'] = 'Kh√¥ng x√°c ƒë·ªãnh'
            
            # 4. T√¨m m√¥ t·∫£ (meta og:description)
            meta_desc = soup.find('meta', property='og:description')
            if meta_desc:
                desc = meta_desc.get('content', '')
                info['bio'] = desc[:200] + '...' if len(desc) > 200 else desc
            
            # 5. T√¨m th√¥ng tin c∆° b·∫£n t·ª´ c√°c div
            # L∆ØU √ù: C·∫•u tr√∫c HTML c·ªßa Facebook THAY ƒê·ªîI TH∆Ø·ªúNG XUY√äN
            # B·∫°n c·∫ßn t·ª± c·∫≠p nh·∫≠t c√°c selector n√†y
            
            # V√≠ d·ª• t√¨m s·ªë ng∆∞·ªùi theo d√µi (n·∫øu l√† trang c√¥ng khai)
            followers_text = ''
            for span in soup.find_all('span'):
                text = span.get_text()
                if 'ng∆∞·ªùi theo d√µi' in text.lower() or 'followers' in text.lower():
                    followers_text = text
                    break
            
            info['followers'] = followers_text if followers_text else 'Kh√¥ng c√¥ng khai'
            
            # 6. X√°c ƒë·ªãnh verified (tick xanh)
            verified = soup.find('i', {'aria-label': True})
            info['verified'] = 'C√≥' if verified and 'ƒë√£ x√°c minh' in verified.get('aria-label', '').lower() else 'Kh√¥ng'
            
            # Th√™m timestamp
            info['scraped_at'] = time.strftime("%d/%m/%Y %H:%M:%S")
            
            return info
            
        except TimeoutException:
            return self._create_error_response("Timeout khi t·∫£i trang")
        except Exception as e:
            print(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}")
            return self._create_error_response(f"L·ªói: {str(e)}")
    
    def scrape_via_graph_api(self, user_id):
        """
        Th·ª≠ l·∫•y th√¥ng tin qua Facebook Graph API
        Y√äU C·∫¶U: Access Token v√† quy·ªÅn truy c·∫≠p
        """
        # B·∫†N C·∫¶N T·ª∞ T·∫†O APP TR√äN DEVELOPERS.FACEBOOK.COM
        access_token = os.environ.get('FB_ACCESS_TOKEN', '')
        
        if not access_token:
            return {'error': 'Ch∆∞a c·∫•u h√¨nh Facebook Access Token'}
        
        try:
            url = f"https://graph.facebook.com/v18.0/{user_id}"
            params = {
                'fields': 'id,name,first_name,last_name',
                'access_token': access_token
            }
            
            response = requests.get(url, params=params, timeout=10)
            data = response.json()
            
            if 'error' in data:
                return {'error': data['error']['message']}
            
            return {
                'success': True,
                'source': 'graph_api',
                'data': data
            }
            
        except Exception as e:
            return {'error': f"API Error: {str(e)}"}
    
    def _create_error_response(self, message):
        """T·∫°o response th√¥ng b√°o l·ªói"""
        return {
            'success': False,
            'error': message,
            'timestamp': time.strftime("%d/%m/%Y %H:%M:%S")
        }
    
    def close(self):
        """ƒê√≥ng tr√¨nh duy·ªát"""
        if self.driver:
            self.driver.quit()
            self.driver = None

# H√†m wrapper ƒë∆°n gi·∫£n ƒë·ªÉ bot.py g·ªçi
def get_facebook_info_real(input_data):
    """
    H√†m ch√≠nh ƒë·ªÉ l·∫•y th√¥ng tin Facebook
    C√≥ th·ªÉ nh·∫≠n username ho·∫∑c UID
    """
    scraper = FacebookScraper(headless=True)
    
    try:
        # X√°c ƒë·ªãnh lo·∫°i input
        if input_data.isdigit():
            # N·∫øu l√† s·ªë, th·ª≠ d√πng Graph API tr∆∞·ªõc
            result = scraper.scrape_via_graph_api(input_data)
            if result.get('success'):
                return result
        
        # M·∫∑c ƒë·ªãnh d√πng web scraping v·ªõi username
        result = scraper.scrape_basic_info(input_data)
        return result
        
    finally:
        scraper.close()